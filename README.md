# ğŸš— Plateforme de Scraping - Expat Dakar

Une application Streamlit moderne pour la collecte, l'analyse et le tÃ©lÃ©chargement de donnÃ©es automobiles depuis Expat-Dakar.

## ğŸ¯ FonctionnalitÃ©s

### ğŸ“‹ Page d'Accueil
- PrÃ©sentation claire de la plateforme
- Description des fonctionnalitÃ©s principales
- Interface moderne et intuitive

### ğŸ•·ï¸ Scraping de DonnÃ©es
- Configuration flexible des paramÃ¨tres de scraping
- Support multi-pages
- Barre de progression en temps rÃ©el
- Gestion d'erreurs robuste

### ğŸ“¥ TÃ©lÃ©chargement
- AccÃ¨s aux donnÃ©es dÃ©jÃ  scrapÃ©es
- TÃ©lÃ©chargement au format CSV
- Informations dÃ©taillÃ©es sur chaque fichier

### ğŸ“Š Dashboard Analytique
- Visualisations interactives avec Plotly
- MÃ©triques clÃ©s (prix moyen, annÃ©e moyenne, etc.)
- Graphiques de distribution et de rÃ©partition
- Support des donnÃ©es scrapÃ©es rÃ©cemment et stockÃ©es

### ğŸ“ Formulaire d'Ã‰valuation
- Ã‰valuation globale de l'application
- Notation par fonctionnalitÃ©
- Commentaires et suggestions
- Sauvegarde des retours utilisateur

## ğŸš€ Installation et Utilisation

### PrÃ©requis
- Python 3.7+
- pip

### Installation

1. **Cloner ou tÃ©lÃ©charger le projet**
```bash
git clone <votre-repo>
cd examen-data-collection
```

2. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

3. **Lancer l'application**
```bash
streamlit run my_data_app.py
```

4. **AccÃ©der Ã  l'application**
Ouvrez votre navigateur et allez sur `http://localhost:8501`

## ğŸ“ Structure du Projet

```
examen-data-collection/
â”œâ”€â”€ my_data_app.py          # Application principale
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â”œâ”€â”€ README.md              # Documentation
â””â”€â”€ data/                  # DonnÃ©es scrapÃ©es
    â”œâ”€â”€ motos_scooters1.csv
    â”œâ”€â”€ motos_scooters2.csv
    â”œâ”€â”€ motos_scooters3.csv
    â”œâ”€â”€ motos_scooters4.csv
    â””â”€â”€ motos_scooters5.csv
```

## ğŸ› ï¸ Architecture du Code

### Structure Modulaire
Le code est organisÃ© en fonctions modulaires pour faciliter la maintenance :

- **`main()`** : Point d'entrÃ©e principal avec navigation
- **`show_home()`** : Page d'accueil
- **`show_scraping()`** : Interface de scraping
- **`show_download()`** : Gestion des tÃ©lÃ©chargements
- **`show_dashboard()`** : Dashboard analytique
- **`show_feedback()`** : Formulaire d'Ã©valuation

### Fonctions Utilitaires
- **`scrape_motos_data()`** : Logique de scraping
- **`clean_data()`** : Nettoyage des donnÃ©es
- **`create_dashboard()`** : CrÃ©ation des visualisations
- **`download_csv()`** : GÃ©nÃ©ration des liens de tÃ©lÃ©chargement

## ğŸ¨ Interface Utilisateur

### Design Moderne
- Interface responsive avec CSS personnalisÃ©
- Navigation par sidebar intuitive
- Cartes d'information stylisÃ©es
- Boutons avec effets de survol

### ExpÃ©rience Utilisateur
- Navigation fluide entre les pages
- Feedback visuel pour toutes les actions
- Barres de progression pour les opÃ©rations longues
- Messages d'erreur et de succÃ¨s clairs

## ğŸ”§ Personnalisation

### Modifier le Scraping
Pour adapter le scraping Ã  vos besoins, modifiez la fonction `scrape_motos_data()` :

```python
def scrape_motos_data(url, max_pages=5):
    # Remplacez la simulation par votre logique de scraping rÃ©elle
    # Utilisez requests et BeautifulSoup pour scraper le site
    pass
```

### Ajouter de Nouvelles Visualisations
Dans la fonction `create_dashboard()`, ajoutez vos graphiques :

```python
# Exemple d'ajout d'un nouveau graphique
fig_new = px.scatter(df_clean, x='Prix_Numerique', y='Kilometrage_Numerique')
st.plotly_chart(fig_new, use_container_width=True)
```

### Personnaliser le Style
Modifiez la section CSS dans le code pour changer l'apparence :

```python
st.markdown("""
<style>
    .main-header {
        /* Vos styles personnalisÃ©s */
    }
</style>
""", unsafe_allow_html=True)
```

## ğŸ“Š DÃ©ploiement sur Streamlit Cloud

1. **PrÃ©parer le projet**
   - Assurez-vous que tous les fichiers sont dans le repository
   - VÃ©rifiez que `requirements.txt` est Ã  jour

2. **DÃ©ployer sur Streamlit Cloud**
   - Connectez votre repository GitHub Ã  Streamlit Cloud
   - Configurez le chemin vers `my_data_app.py`
   - DÃ©ployez l'application

3. **Configuration avancÃ©e**
   - Ajoutez des variables d'environnement si nÃ©cessaire
   - Configurez les permissions d'accÃ¨s aux donnÃ©es

## ğŸ› DÃ©pannage

### ProblÃ¨mes Courants

**Erreur de dÃ©pendances :**
```bash
pip install --upgrade -r requirements.txt
```

**ProblÃ¨me de port :**
```bash
streamlit run my_data_app.py --server.port 8502
```

**Fichiers de donnÃ©es manquants :**
- VÃ©rifiez que le dossier `data/` existe
- Assurez-vous que les fichiers CSV sont prÃ©sents

## ğŸ¤ Contribution

Pour contribuer au projet :

1. Fork le repository
2. CrÃ©ez une branche pour votre fonctionnalitÃ©
3. Committez vos changements
4. Poussez vers la branche
5. CrÃ©ez une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- Ouvrez une issue sur GitHub
- Contactez l'Ã©quipe de dÃ©veloppement

---

**DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© de scraping de donnÃ©es**