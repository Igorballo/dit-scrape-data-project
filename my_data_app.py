import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import base64
import io
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
from selenium import webdriver 
from selenium.webdriver.common.by import By 


# Instantier l'objet chrome options
options = webdriver.ChromeOptions() 
# d√©finir l'option d'utiliser chrome en mode headless ( utiliser afin de lancer le script en background)
options.add_argument("--headless=new") 
# initialiser l'instance de chrome driver en mode headless
driver = webdriver.Chrome(options=options)


# Configuration de la page
st.set_page_config(
    page_title="Plateforme de Scraping - Expat Dakar",
    page_icon="üöó",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√© pour une meilleure apparence
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .feature-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin: 1rem 0;
        border-left: 4px solid #667eea;
    }
    
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 0.5rem;
    }
    
    .stButton > button {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 0.75rem 2rem;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    }
    
    .sidebar .sidebar-content {
        background: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)

# Initialisation des variables de session
if 'scraped_data' not in st.session_state:
    st.session_state.scraped_data = []
if 'current_page' not in st.session_state:
    st.session_state.current_page = "Accueil"

# Fonction pour t√©l√©charger un fichier CSV
def download_csv(dataframe, filename):
    csv = dataframe.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}.csv" class="download-link">üì• T√©l√©charger {filename}</a>'
    return href

# Fonction de scraping (exemple pour motos)
def scrape_motos_data(url, max_pages=5):
    """Fonction de scraping pour les donn√©es de motos"""
    scraped_data = []
    
    with st.spinner(f'Scraping en cours... Veuillez patienter...'):
        for page in range(1, max_pages + 1):
            try:
                # Simulation de scraping (remplacez par votre logique r√©elle)
                page_url = f"{url}?page={page}"
                
                # Simulation de donn√©es scrap√©es
                for i in range(10):
                    scraped_data.append({
                        'Titre': f'Moto {page}-{i+1}',
                        'Prix': f'{50000 + (page * 1000) + (i * 500)} FCFA',
                        'Marque': f'Marque {i % 5 + 1}',
                        'Ann√©e': f'{2015 + (i % 8)}',
                        'Kilom√©trage': f'{10000 + (i * 5000)} km',
                        'Localisation': 'Dakar',
                        'Date_Scraping': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                
                # Barre de progression
                progress = page / max_pages
                st.progress(progress)
                time.sleep(0.5)  # Simulation du temps de scraping
                
            except Exception as e:
                st.error(f"Erreur lors du scraping de la page {page}: {str(e)}")
                break

    return pd.DataFrame(scraped_data)

def scrape_voitures_data(url, max_pages):
    """Fonction de scraping pour les donn√©es de voitures"""
    scraped_data = []
    
    print(f"Scraping voitures: {url}")
    with st.spinner(f'Scraping voitures en cours... Veuillez patienter...'):
        for page in range(1, max_pages + 1):
            try:
                # Simulation de scraping pour voitures
                page_url = f"{url}?page={page}"
                
                # Simulation de donn√©es scrap√©es
                for i in range(10):
                    scraped_data.append({
                        'Titre': f'Voiture {page}-{i+1}',
                        'Prix': f'{2000000 + (page * 50000) + (i * 25000)} FCFA',
                        'Marque': f'Marque {i % 8 + 1}',
                        'Ann√©e': f'{2010 + (i % 12)}',
                        'Kilom√©trage': f'{50000 + (i * 10000)} km',
                        'Localisation': 'Dakar',
                        'Date_Scraping': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                
                # Barre de progression
                progress = page / max_pages
                st.progress(progress)
                time.sleep(0.5)  # Simulation du temps de scraping
                
            except Exception as e:
                st.error(f"Erreur lors du scraping de la page {page}: {str(e)}")
                break

    return pd.DataFrame(scraped_data)

def scrape_location_data(url, max_pages):
    """Fonction de scraping pour les donn√©es de location de voitures"""
    scraped_data = []
    
    print(f"Scraping location: {url}")
    with st.spinner(f'Scraping location en cours... Veuillez patienter...'):
        for page in range(1, max_pages + 1):
            try:
                # Simulation de scraping pour location
                page_url = f"{url}?page={page}"
                
                # Simulation de donn√©es scrap√©es
                for i in range(10):
                    scraped_data.append({
                        'Titre': f'Location {page}-{i+1}',
                        'Prix': f'{15000 + (page * 1000) + (i * 500)} FCFA/jour',
                        'Marque': f'Marque {i % 6 + 1}',
                        'Ann√©e': f'{2015 + (i % 8)}',
                        'Kilom√©trage': f'{80000 + (i * 15000)} km',
                        'Localisation': 'Dakar',
                        'Date_Scraping': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                
                # Barre de progression
                progress = page / max_pages
                st.progress(progress)
                time.sleep(0.5)  # Simulation du temps de scraping
                
            except Exception as e:
                st.error(f"Erreur lors du scraping de la page {page}: {str(e)}")
                break

    return pd.DataFrame(scraped_data)


# Fonction pour nettoyer les donn√©es
def clean_data(dataframe):
    """Fonction pour nettoyer les donn√©es scrap√©es"""
    if dataframe.empty:
        return dataframe
    
    # Copie des donn√©es
    df_clean = dataframe.copy()
    
    # Nettoyage des prix (suppression de 'FCFA' et conversion en num√©rique)
    df_clean['Prix_Numerique'] = df_clean['Prix'].str.replace(' FCFA', '').str.replace(' ', '').astype(float)
    
    # Nettoyage du kilom√©trage
    df_clean['Kilometrage_Numerique'] = df_clean['Kilom√©trage'].str.replace(' km', '').str.replace(' ', '').astype(float)
    
    # Conversion de l'ann√©e en num√©rique
    df_clean['Annee_Numerique'] = df_clean['Ann√©e'].astype(int)
    
    return df_clean

# Fonction pour cr√©er des visualisations
def create_dashboard(dataframe):
    """Cr√©ation du dashboard avec des graphiques"""
    if dataframe.empty:
        st.warning("Aucune donn√©e √† afficher dans le dashboard.")
        return
    
    # Nettoyage des donn√©es pour le dashboard
    df_clean = clean_data(dataframe)
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Nombre total", len(dataframe))
    
    with col2:
        avg_price = df_clean['Prix_Numerique'].mean()
        st.metric("Prix moyen", f"{avg_price:,.0f} FCFA")
    
    with col3:
        avg_year = df_clean['Annee_Numerique'].mean()
        st.metric("Ann√©e moyenne", f"{avg_year:.0f}")
    
    with col4:
        avg_km = df_clean['Kilometrage_Numerique'].mean()
        st.metric("Km moyen", f"{avg_km:,.0f} km")
    
    # Graphiques
    col1, col2 = st.columns(2)
    
    with col1:
        # Distribution des prix
        fig_price = px.histogram(df_clean, x='Prix_Numerique', 
                               title='Distribution des Prix',
                               labels={'Prix_Numerique': 'Prix (FCFA)', 'count': 'Nombre'})
        st.plotly_chart(fig_price, use_container_width=True)
    
    with col2:
        # Distribution des ann√©es
        fig_year = px.bar(df_clean['Annee_Numerique'].value_counts().reset_index(),
                         x='index', y='Annee_Numerique',
                         title='Distribution par Ann√©e',
                         labels={'index': 'Ann√©e', 'Annee_Numerique': 'Nombre'})
        st.plotly_chart(fig_year, use_container_width=True)
    
    # Graphique des marques
    fig_brand = px.pie(df_clean, names='Marque', title='R√©partition par Marque')
    st.plotly_chart(fig_brand, use_container_width=True)

# Page d'accueil
def show_home():
    st.markdown("""
    <div class="main-header">
        <h1>üöó Plateforme de Scraping - Expat Dakar</h1>
        <p>Votre solution compl√®te pour la collecte et l'analyse de donn√©es automobiles</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    <div class="feature-card">
        <h3>üéØ Notre Mission</h3>
        <p>Cette plateforme vous permet de collecter, analyser et t√©l√©charger des donn√©es automobiles 
        depuis Expat-Dakar de mani√®re simple et efficace. Que vous soyez un particulier cherchant 
        une moto ou un analyste de donn√©es, notre outil vous accompagne dans votre d√©marche.</p>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="feature-card">
            <h4>üï∑Ô∏è Scraping Intelligent</h4>
            <p>Collectez automatiquement les donn√©es de plusieurs pages avec notre outil de scraping avanc√©.</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="feature-card">
            <h4>üìä Dashboard Analytique</h4>
            <p>Visualisez vos donn√©es avec des graphiques interactifs et des m√©triques cl√©s.</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="feature-card">
            <h4>üì• T√©l√©chargement Facile</h4>
            <p>T√©l√©chargez vos donn√©es au format CSV pour une utilisation ult√©rieure.</p>
        </div>
        """, unsafe_allow_html=True)

# Page de scraping
def show_scraping():
    st.markdown("<h2>üï∑Ô∏è Scraping de Donn√©es</h2>", unsafe_allow_html=True)
    
    st.markdown("""
    <div class="feature-card">
        <p>Configurez vos param√®tres de scraping et lancez la collecte de donn√©es depuis Expat-Dakar.</p>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("Configuration")
        url = st.selectbox("URL de base", options=["https://dakar-auto.com/senegal/voitures-4", "https://dakar-auto.com/senegal/motos-and-scooters-3", "https://dakar-auto.com/senegal/location-de-voitures-19"], index=0)
        
        max_pages = st.number_input("Nombre de pages √† scraper", value=1, min_value=1, step=1)
        
        if st.button("üöÄ Lancer le Scraping", key="scrape_btn"):
            print(f"Scraping igor: {url}")
            if url and url == 'https://dakar-auto.com/senegal/voitures-4':
                print(f"Scraping igor v2: {url}")
                scraped_df = scrape_voitures_data(url, max_pages)
                st.session_state.scraped_data = scraped_df
                st.success(f"‚úÖ Scraping termin√© ! {len(scraped_df)} donn√©es collect√©es.")
            elif url and url == 'https://dakar-auto.com/senegal/motos-and-scooters-3':
                scraped_df = scrape_motos_data(url, max_pages)
                st.session_state.scraped_data = scraped_df
                st.success(f"‚úÖ Scraping termin√© ! {len(scraped_df)} donn√©es collect√©es.")
            elif url and url == 'https://dakar-auto.com/senegal/location-de-voitures-19':
                scraped_df = scrape_location_data(url, max_pages)
                st.session_state.scraped_data = scraped_df
            else:
                print(f"Scraping igor v3: {url}")
                st.error("Veuillez entrer une URL valide.")
    
    with col2:
        st.subheader("Donn√©es Scrap√©es")
        if st.session_state.scraped_data:
            st.dataframe(st.session_state.scraped_data.head())
            
            # Bouton de t√©l√©chargement
            csv_link = download_csv(st.session_state.scraped_data, "donnees_scrapees")
            st.markdown(csv_link, unsafe_allow_html=True)
        else:
            st.info("Aucune donn√©e scrap√©e pour le moment. Lancez le scraping pour commencer.")

# Page de t√©l√©chargement
def show_download():
    st.markdown("<h2>üì• T√©l√©chargement de Donn√©es</h2>", unsafe_allow_html=True)
    
    st.markdown("""
    <div class="feature-card">
        <p>T√©l√©chargez les donn√©es d√©j√† scrap√©es et stock√©es dans notre base de donn√©es.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Liste des fichiers disponibles
    available_files = [
        ('motos_scooters1.csv', 'Motocycles - Lot 1'),
        ('motos_scooters2.csv', 'Motocycles - Lot 2'),
        ('motos_scooters3.csv', 'Motocycles - Lot 3'),
        ('motos_scooters4.csv', 'Motocycles - Lot 4'),
        ('motos_scooters5.csv', 'Motocycles - Lot 5')
    ]
    
    st.subheader("Fichiers Disponibles")
    
    for filename, description in available_files:
        try:
            df = pd.read_csv(f'data/{filename}')
            
            col1, col2, col3, col4 = st.columns([3, 2, 2, 2])
            
            with col1:
                st.write(f"**{description}**")
            
            with col2:
                st.write(f"üìä {df.shape[0]} lignes")
            
            with col3:
                st.write(f"üìã {df.shape[1]} colonnes")
            
            with col4:
                csv_link = download_csv(df, filename.replace('.csv', ''))
                st.markdown(csv_link, unsafe_allow_html=True)
            
            st.divider()
            
        except FileNotFoundError:
            st.warning(f"Fichier {filename} non trouv√©.")

# Page dashboard
def show_dashboard():
    st.markdown("<h2>üìä Dashboard Analytique</h2>", unsafe_allow_html=True)
    
    st.markdown("""
    <div class="feature-card">
        <p>Visualisez et analysez vos donn√©es avec des graphiques interactifs et des m√©triques cl√©s.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # S√©lection des donn√©es √† analyser
    data_source = st.radio(
        "Choisissez la source de donn√©es :",
        ["Donn√©es scrap√©es r√©cemment", "Donn√©es stock√©es"]
    )
    
    if data_source == "Donn√©es scrap√©es r√©cemment":
        if st.session_state.scraped_data:
            create_dashboard(st.session_state.scraped_data)
        else:
            st.info("Aucune donn√©e scrap√©e r√©cemment. Allez dans l'onglet Scraping pour collecter des donn√©es.")
    else:
        # Charger et combiner toutes les donn√©es stock√©es
        all_data = []
        for i in range(1, 6):
            try:
                df = pd.read_csv(f'data/motos_scooters{i}.csv')
                all_data.append(df)
            except FileNotFoundError:
                continue
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            create_dashboard(combined_df)
        else:
            st.warning("Aucune donn√©e stock√©e trouv√©e.")

# Page formulaire d'√©valuation
def show_feedback():
    st.markdown("<h2>üìù √âvaluation de l'Application</h2>", unsafe_allow_html=True)
    
    st.markdown("""
    <div class="feature-card">
        <p>Aidez-nous √† am√©liorer notre plateforme en partageant votre exp√©rience utilisateur. 
        Utilisez le formulaire ci-dessous pour nous donner votre avis.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Int√©gration du formulaire KoboToolbox
    st.markdown("""
    <div style="display: flex; justify-content: center; margin: 2rem 0;">
        <iframe src="https://ee.kobotoolbox.org/i/kXR3jNKQ" 
                width="100%" 
                height="700" 
                frameborder="0" 
                style="border-radius: 10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
        </iframe>
    </div>
    """, unsafe_allow_html=True)

# Navigation principale
def main():
    # Sidebar pour la navigation
    st.sidebar.markdown("""
    <div style="text-align: center; padding: 1rem;">
        <h3>üöó Navigation</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # Boutons de navigation
    if st.sidebar.button("üè† Accueil", key="nav_home"):
        st.session_state.current_page = "Accueil"
    
    if st.sidebar.button("üï∑Ô∏è Scraping", key="nav_scraping"):
        st.session_state.current_page = "Scraping"
    
    if st.sidebar.button("üì• T√©l√©chargement", key="nav_download"):
        st.session_state.current_page = "T√©l√©chargement"
    
    if st.sidebar.button("üìä Dashboard", key="nav_dashboard"):
        st.session_state.current_page = "Dashboard"
    
    if st.sidebar.button("üìù √âvaluation", key="nav_feedback"):
        st.session_state.current_page = "√âvaluation"
    
    st.sidebar.markdown("---")
    
    # Informations sur l'application
    st.sidebar.markdown("""
    <div style="padding: 1rem;">
        <h4>‚ÑπÔ∏è √Ä propos</h4>
        <p>Plateforme de scraping et d'analyse de donn√©es automobiles.</p>
        <p><strong>Version:</strong> 1.0</p>
        <p><strong>Source:</strong> Expat-Dakar</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Affichage de la page s√©lectionn√©e
    if st.session_state.current_page == "Accueil":
        show_home()
    elif st.session_state.current_page == "Scraping":
        show_scraping()
    elif st.session_state.current_page == "T√©l√©chargement":
        show_download()
    elif st.session_state.current_page == "Dashboard":
        show_dashboard()
    elif st.session_state.current_page == "√âvaluation":
        show_feedback()

if __name__ == "__main__":
    main()




 


